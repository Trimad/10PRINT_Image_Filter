{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESSDN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trimad/10PRINT_Image_Filter/blob/master/ESSDN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extremely Simple Stable Diffusion Notebook**\n",
        "\n",
        "For the ones that aren't tech savvy.\n",
        "\n",
        "There will be input boxes where you have to input information.\n",
        "\n",
        "In order to view the images, you need to navegate through the folders and go to forks/stable-diffusion-main/outputs/txt2img-samples/samples or img2img-samples/samples if you are doing img2img.\n",
        "\n",
        "![chrome_pGidEPqQWM.png](data:image/png;base64,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)\n",
        "\n",
        "Double click on the files to show them on the right side."
      ],
      "metadata": {
        "id": "KPcZyP6_Pj2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "nAOF-dNuLFOL",
        "outputId": "88bd9d51-f0d9-4f56-99e0-4c66b5fb29f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From this point forward, we are going to install packages/verify packages, do you agree? (Y/N): Y\n",
            "It is Highly recommended to use Python 3.7.9+\n",
            "Many dependencies such as OpenCV only work on specific versions that are only available for Python 3.7.\n",
            "Current Python Version: 3.7.13\n",
            "You can press Control+C to exit the program. There will be no more annoncements about this.\n",
            "Press Enter to Continue...\n",
            "A new folder is going to be created to store everything needed. Please check that this directory is the one you want.\n",
            "Current working directory:/content\n",
            "Continue? (Y/N): Y\n",
            "Creating /forks\n",
            "Downloading Official Repository. CompVis/stable-diffusion\n",
            "Downloading main.zip\n",
            "Unzipping File\n",
            "File unzipped, removing main.zip\n",
            "Creating folders\n",
            "Copying configuration file v1-inference.yaml\n",
            "Downloading and Instaling dependencies...\n",
            "Downloading and installing github dependencies\n",
            "Downloading tmp/taming-transformers.zip\n",
            "Downloading tmp/CLIP.zip\n",
            "Installing project dependencies\n",
            "Downloading weights\n",
            "Be patient as this might take a while. If this crashes here, look for another direct download and replace the link in the script. Delete the forks folder and restart the process (dependencies will stay)\n",
            "input your download link (mediafire doesn't works btw): https://drinkordiecdn.lol/sd-v1-3-full-ema.ckpt\n",
            "Downloading model.ckpt\n",
            "[==================================================]Download Finished, moving weights.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: taming-transformers in ./tmp/taming-transformers-master (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->taming-transformers) (4.1.1)\n",
            "Part 1 Installation finished\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "#@markdown Press the play button to start.\n",
        "import subprocess\n",
        "import sys\n",
        "import platform\n",
        "def instpkg(packagename):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', packagename])\n",
        "response = input(\"From this point forward, we are going to install packages/verify packages, do you agree? (Y/N): \")\n",
        "if response == \"N\":\n",
        "    exit()\n",
        "instpkg(\"requests\")\n",
        "instpkg(\"pyuac\")\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "def download_file(url, filename):\n",
        "    link = url\n",
        "    file_name = filename\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        print(\"Downloading %s\" % file_name)\n",
        "        response = requests.get(link, stream=True)\n",
        "        total_length = response.headers.get('content-length')\n",
        "\n",
        "        if total_length is None: # no content length header\n",
        "            f.write(response.content)\n",
        "        else:\n",
        "            dl = 0\n",
        "            total_length = int(total_length)\n",
        "            for data in response.iter_content(chunk_size=4096):\n",
        "                dl += len(data)\n",
        "                f.write(data)\n",
        "                done = int(50 * dl / total_length)\n",
        "                sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )    \n",
        "                sys.stdout.flush()\n",
        "\n",
        "def unzip(file_to_unzip, path_to_extract):\n",
        "    with zipfile.ZipFile(file_to_unzip, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(path_to_extract)\n",
        "\n",
        "print(\"It is Highly recommended to use Python 3.7.9+\")\n",
        "print(\"Many dependencies such as OpenCV only work on specific versions that are only available for Python 3.7.\")\n",
        "print(\"Current Python Version: \" + platform.python_version())\n",
        "print(\"You can press Control+C to exit the program. There will be no more annoncements about this.\")\n",
        "input(\"Press Enter to Continue...\")\n",
        "\n",
        "print(\"A new folder is going to be created to store everything needed. Please check that this directory is the one you want.\")\n",
        "print(\"Current working directory:\" + os.getcwd())\n",
        "response = input(\"Continue? (Y/N): \")\n",
        "if response == \"N\":\n",
        "    exit()\n",
        "\n",
        "print(\"Creating /forks\")\n",
        "os.mkdir(\"forks\")\n",
        "os.chdir(\"forks\")\n",
        "print(\"Downloading Official Repository. CompVis/stable-diffusion\")\n",
        "download_file(\"https://github.com/CompVis/stable-diffusion/archive/refs/heads/main.zip\", \"main.zip\")\n",
        "print(\"Unzipping File\")\n",
        "unzip(\"main.zip\", \"./\")\n",
        "print(\"File unzipped, removing main.zip\")\n",
        "os.remove(\"main.zip\")\n",
        "os.chdir(\"stable-diffusion-main\")\n",
        "print(\"Creating folders\")\n",
        "os.mkdir(\"scripts/configs\")\n",
        "os.mkdir(\"scripts/configs/stable-diffusion\")\n",
        "print(\"Copying configuration file v1-inference.yaml\")\n",
        "shutil.copy(\"configs/stable-diffusion/v1-inference.yaml\", \"scripts/configs/stable-diffusion/v1-inference.yaml\")\n",
        "#Please check spelling\n",
        "print(\"Downloading and Instaling dependencies...\")\n",
        "instpkg(\"albumentations==0.4.3\")\n",
        "instpkg(\"opencv-python==4.1.2.30\")\n",
        "instpkg(\"pudb==2019.2\")\n",
        "instpkg(\"imageio==2.9.0\")\n",
        "instpkg(\"imageio-ffmpeg==0.4.2\")\n",
        "#instpkg(\"pytorch-lightning==1.4.2\")\n",
        "instpkg(\"pytorch-lightning\")\n",
        "instpkg(\"omegaconf==2.1.1\")\n",
        "instpkg(\"test-tube>=0.7.5\")\n",
        "instpkg(\"streamlit>=0.73.1\")\n",
        "instpkg(\"einops==0.3.0\")\n",
        "instpkg(\"torch-fidelity==0.3.0\")\n",
        "instpkg(\"transformers==4.19.2\")\n",
        "instpkg(\"kornia\")\n",
        "\n",
        "print(\"Downloading and installing github dependencies\")\n",
        "os.mkdir(\"tmp\")\n",
        "download_file(\"https://github.com/CompVis/taming-transformers/archive/refs/heads/master.zip\", \"tmp/taming-transformers.zip\")\n",
        "download_file(\"https://github.com/openai/CLIP/archive/refs/heads/main.zip\", \"tmp/CLIP.zip\")\n",
        "unzip(\"tmp/taming-transformers.zip\", \"tmp\")\n",
        "unzip(\"tmp/CLIP.zip\", \"tmp\")\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-e', 'tmp/taming-transformers-master'])\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-e', 'tmp/CLIP-main'])\n",
        "\n",
        "print(\"Installing project dependencies\")\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-e', '.'])\n",
        "\n",
        "print(\"Downloading weights\")\n",
        "print(\"Be patient as this might take a while. If this crashes here, look for another direct download and replace the link in the script. Delete the forks folder and restart the process (dependencies will stay)\")\n",
        "THIS_IS_THE_DOWNLOAD_LINK = input(\"input your download link (mediafire doesn't works btw): \")\n",
        "download_file(THIS_IS_THE_DOWNLOAD_LINK, \"model.ckpt\")\n",
        "print(\"Download Finished, moving weights.\")\n",
        "os.mkdir(\"models/ldm/stable-diffusion-v1\")\n",
        "os.replace(\"model.ckpt\", \"models/ldm/stable-diffusion-v1/model.ckpt\")\n",
        "!python -m pip install taming-transformers\n",
        "print(\"Part 1 Installation finished\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#@markdown Once the first cell has finished. Press this play button.\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "os.chdir(\"/content/forks/stable-diffusion-main\")\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class Config():\n",
        "    def __init__(self):        \n",
        "        self.outdir = 'outputs/txt2img-samples'\n",
        "        self.ddim_steps = 50\n",
        "        self.plms = False\n",
        "\n",
        "        self.init_img = None\n",
        "        \n",
        "        self.laion400m = False\n",
        "        self.seed = 42\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ckpt = 'models/ldm/stable-diffusion-v1/model.ckpt'\n",
        "        self.precision = 'autocast'\n",
        "        self.n_rows = 0\n",
        "        self.fixed_code = True\n",
        "        self.from_file = False\n",
        "        self.C = 4\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.f = 8\n",
        "        self.n_samples = 1\n",
        "        self.n_iter = 1\n",
        "        self.scale = 7.5\n",
        "        self.ddim_eta = 0.0\n",
        "        self.skip_save = False\n",
        "        self.skip_grid = False\n",
        "\n",
        "opt = Config()\n",
        "\n",
        "if opt.laion400m:\n",
        "        print(\"Falling back to LAION 400M model...\")\n",
        "        opt.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
        "        opt.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
        "        opt.outdir = \"outputs/txt2img-samples-laion400m\"\n",
        "\n",
        "seed_everything(opt.seed)\n",
        "\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "model = model.half()\n",
        "\n",
        "def txt2img(opt, model):\n",
        "    device = 'cuda'\n",
        "    if opt.plms:\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "    images = []\n",
        "    if not opt.from_file:\n",
        "\n",
        "        assert opt.prompt is not None\n",
        "        data = [batch_size * [opt.prompt]]\n",
        "\n",
        "    else:\n",
        "        print(f\"reading prompts from {opt.from_file}\")\n",
        "        with open(opt.from_file, \"r\") as f:\n",
        "            data = f.read().splitlines()\n",
        "            data = list(chunk(data, batch_size))\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                #tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "                        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                         conditioning=c,\n",
        "                                                         batch_size=opt.n_samples,\n",
        "                                                         shape=shape,\n",
        "                                                         verbose=False,\n",
        "                                                         unconditional_guidance_scale=opt.scale,\n",
        "                                                         unconditional_conditioning=uc,\n",
        "                                                         eta=opt.ddim_eta,\n",
        "                                                         x_T=start_code)\n",
        "\n",
        "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if not opt.skip_save:\n",
        "                            for x_sample in x_samples_ddim:\n",
        "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                                images[-1].save(\n",
        "                                    os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
        "                                base_count += 1\n",
        "\n",
        "                        if not opt.skip_grid:\n",
        "                            all_samples.append(x_samples_ddim)\n",
        "\n",
        "                if not opt.skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    image = Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "                print(f'Your images are here {outpath}')\n",
        "    return images\n",
        "\n",
        "\n",
        "print(\"Everything is ready!\")\n",
        "print(\"self.n_samples set to 1, you can change it in line 67\")\n",
        "print(\"Please select one of the options:\")\n",
        "while True:\n",
        "    print(\"[1] txt2img\")\n",
        "    print(\"[2] img2img\")\n",
        "    selected = input(\"type a number:\")\n",
        "    if selected == \"1\":\n",
        "        opt = Config()\n",
        "        opt.plms = True\n",
        "        # Uncomment to modify parameters\n",
        "        #opt.ddim_steps = 70\n",
        "        #opt.scale = 10.0\n",
        "        #opt.n_iter = 1\n",
        "        #opt.n_samples = 1\n",
        "        #opt.seed = 1234; seed_everything(opt.seed)\n",
        "        opt.prompt = input(\"Type the desired prompt: \")\n",
        "        images = txt2img(opt, model)\n",
        "    if selected == \"2\":\n",
        "        import PIL\n",
        "        from einops import repeat\n",
        "        def load_img(path):\n",
        "            image = Image.open(path).convert(\"RGB\")\n",
        "            w, h = image.size\n",
        "            print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "            w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "            image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "            image = np.array(image).astype(np.float32) / 255.0\n",
        "            image = image[None].transpose(0, 3, 1, 2)\n",
        "            image = torch.from_numpy(image)\n",
        "            return 2.*image - 1.\n",
        "\n",
        "        def img2img(opt, model):\n",
        "            device = \"cuda\"\n",
        "            if opt.plms:\n",
        "                raise NotImplementedError(\"PLMS sampler not (yet) supported\")\n",
        "                sampler = PLMSSampler(model)\n",
        "            else:\n",
        "                sampler = DDIMSampler(model)\n",
        "            images = []\n",
        "            os.makedirs(opt.outdir, exist_ok=True)\n",
        "            outpath = opt.outdir\n",
        "\n",
        "            batch_size = opt.n_samples\n",
        "            n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "            if not opt.from_file:\n",
        "                prompt = opt.prompt\n",
        "                assert prompt is not None\n",
        "                data = [batch_size * [prompt]]\n",
        "\n",
        "            else:\n",
        "                print(f\"reading prompts from {opt.from_file}\")\n",
        "                with open(opt.from_file, \"r\") as f:\n",
        "                    data = f.read().splitlines()\n",
        "                    data = list(chunk(data, batch_size))\n",
        "\n",
        "            sample_path = os.path.join(outpath, \"samples\")\n",
        "            os.makedirs(sample_path, exist_ok=True)\n",
        "            base_count = len(os.listdir(sample_path))\n",
        "            grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "            assert os.path.isfile(opt.init_img)\n",
        "            init_image = load_img(opt.init_img).to(device).type(model.dtype)\n",
        "            init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "            init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "            sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "            assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "            t_enc = int(opt.strength * opt.ddim_steps)\n",
        "            print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "            precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "            with torch.no_grad():\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    with model.ema_scope():\n",
        "                        tic = time.time()\n",
        "                        all_samples = list()\n",
        "                        for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                            for prompts in tqdm(data, desc=\"data\"):\n",
        "                                uc = None\n",
        "                                if opt.scale != 1.0:\n",
        "                                    uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                                if isinstance(prompts, tuple):\n",
        "                                    prompts = list(prompts)\n",
        "                                c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                                # encode (scaled latent)\n",
        "                                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                                # decode it\n",
        "                                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                        unconditional_conditioning=uc,)\n",
        "\n",
        "                                x_samples = model.decode_first_stage(samples)\n",
        "                                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                                if not opt.skip_save:\n",
        "                                    for x_sample in x_samples:\n",
        "                                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                        images += [Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                                        images[-1].save(\n",
        "                                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
        "                                        base_count += 1\n",
        "                                all_samples.append(x_samples)\n",
        "\n",
        "                        if not opt.skip_grid:\n",
        "                            # additionally, save as grid\n",
        "                            grid = torch.stack(all_samples, 0)\n",
        "                            grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                            grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                            # to image\n",
        "                            grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                            Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                            grid_count += 1\n",
        "\n",
        "                        toc = time.time()\n",
        "\n",
        "            print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "                f\" \\nEnjoy.\")\n",
        "            return images\n",
        "\n",
        "        opt = Config()\n",
        "        opt.outdir = \"outputs/img2img-samples\"\n",
        "        opt.strength = input(\"Strength: \")\n",
        "        # Uncomment to modify parameters\n",
        "        #opt.scale = 10.0\n",
        "        #opt.n_iter = 1\n",
        "        #opt.n_samples = 1\n",
        "        #opt.seed = 1234; seed_everything(opt.seed)\n",
        "        opt.prompt = input(\"Prompt: \")\n",
        "        opt.init_img = input(\"Path to Input Image:\") # You might want to resize your image (because of odd dimensions?)\n",
        "        images = img2img(opt, model)\n"
      ],
      "metadata": {
        "id": "2LpwpAXoMYji",
        "outputId": "6bd9a69c-7bfc-4c51-97d1-8a8011933964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from models/ldm/stable-diffusion-v1/model.ckpt\n",
            "Global Step: 440000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6b4e9049d0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{opt.config}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{opt.ckpt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6b4e9049d0d8>\u001b[0m in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt, verbose)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Global Step: {pl_sd['global_step']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/forks/stable-diffusion-main/ldm/util.py\u001b[0m in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected key `target` to instantiate.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/forks/stable-diffusion-main/ldm/util.py\u001b[0m in \u001b[0;36mget_obj_from_str\u001b[0;34m(string, reload)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mmodule_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_imp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/content/forks/stable-diffusion-main/ldm/models/diffusion/ddpm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLitEma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormal_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiagonalGaussianDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQModelInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIdentityFirstStage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusionmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_beta_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_into_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDIMSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/forks/stable-diffusion-main/ldm/models/autoencoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorQuantizer2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mVectorQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusionmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'taming'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}